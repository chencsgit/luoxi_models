# -*- coding: utf-8 -*-
# Copyright 2022 The Luoxi Team.
# All rights reserved.
# This source code is licensed under the Apache 2.0 license
# found in the LICENSE file in the root directory.

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class ECRec(nn.Module):
    def __init__(self, args, device):
        super(ECRec, self).__init__()
        self.linear1 = nn.Linear(args.d_model*2 + args.d_memory * 2, 256)
        nn.init.xavier_uniform_(self.linear1.weight, gain=1.0)
        nn.init.zeros_(self.linear1.bias)
        self.linear2 = nn.Linear(256, 128)
        nn.init.xavier_uniform_(self.linear2.weight, gain=1.0)
        nn.init.zeros_(self.linear2.bias)
        self.linear3 = nn.Linear(128, 2)
        nn.init.xavier_uniform_(self.linear3.weight, gain=1.0)
        nn.init.zeros_(self.linear3.bias)
        self.emb = Embedding(args.num_user, args.num_item, args.num_cat, d_model=args.d_model, d_mem=args.d_memory)
        if args.task_type != 'inference':
            self.agg = Aggregation(d_model=args.d_model * 2, num_head=args.num_head)
        self.length = args.length
        self.K = args.K
        self.drop_rate = args.drop_rate
        # self.temp = args.temp
        self.device = device
        self.prelu1 = nn.PReLU()
        self.prelu2 = nn.PReLU()
        self.bn = nn.BatchNorm1d(args.d_model*2 + args.d_memory * 2)
        self.l = args.l
        self.task_type = args.task_type
        self.gate_linear1 = nn.Linear(2, 16)
        self.gate_linear2 = nn.Linear(16, 1)
        self.prelu_gate = nn.PReLU()
        self.gate = args.gate

    def forward(self, input, mask_mem = False):
        bs = input['user_id'].size(0)
        if mask_mem:
            x = torch.cat([torch.zeros([bs,1], dtype=int).to(self.device), input['item_seq']], -1) # (B, 1+T)
        else:
            x = torch.cat([torch.ones([bs,1], dtype=int).to(self.device), input['item_seq']], -1) # (B, 1+T)
        mask_raw = (x > 0).unsqueeze(1).unsqueeze(1) # (B, 1, 1, 1+T)


        res_tmp = self.emb(input)
        mem_raw, mem_cat_raw, seq, item, seq_cat, cat, user, dev_seq, dev_seq_cat = \
            res_tmp['mem_item'], res_tmp['mem_cate'], res_tmp['seq_item'], res_tmp['item'], res_tmp['seq_cate'], res_tmp[
                'cate'], res_tmp['user'], res_tmp['edge_seq_item'], res_tmp['edge_seq_cate']
        ######################################################
        # mem = torch.cat([mem_raw,mem_cat_raw], dim=1) # (B, T, 2D)
        mem_all = torch.cat([mem_raw, mem_cat_raw], -1)  # (B, 2D)
        seq_all = torch.cat([seq, seq_cat], -1)  # (B, T, 2D)
        item_all = torch.cat([item, cat], -1)  # (B, 2D)

        dev_all = torch.cat([dev_seq, dev_seq_cat], -1)  # (B, T, 2D)

        ######################################################

        mask_dev = ((input['edge_item_seq'] > 0) * 1.0)  # (B, T)
        dev_actual_len = torch.sum(mask_dev, -1, keepdim=True)  # (B, 1)
        dev_len = torch.where(dev_actual_len > 0, dev_actual_len, torch.ones_like(dev_actual_len))
        dev_all_sum = torch.sum(dev_all * mask_dev.unsqueeze(-1), 1)
        dev_all_mean = dev_all_sum / dev_len

        ########################################################

        if 'seq_len' in input.keys(): # generated by a neural network
            seq_len = input['seq_len']
            gate = torch.cat([seq_len.unsqueeze(-1).float(), dev_actual_len], dim=-1)
            gate = self.gate_linear2(self.prelu_gate(self.gate_linear1(gate)))
            gate = torch.sigmoid(gate)
        else:
            gate = self.gate # use specific hyperparameter

        scores = []
        '''
        train:     task_type=='train', self.training==True
        eval:      task_type=='train', self.training==False
        inference: task_type=='inference', self.training==False
        update:    task_type=='inference', self.training==True
        '''
        if self.task_type=='train' and self.training:
            mask_meta = torch.ones_like(mask_raw) * (1 - self.drop_rate)
            for k in range(self.K):
                # random mask
                mask = torch.bernoulli(mask_meta) * mask_raw
                mem_all, _ = self.agg(mem_all, seq_all, mask, mode='b')
                all_feas = torch.cat([gate * mem_all + (1 - gate) * dev_all_mean, item_all], -1)
                all_feas = self.bn(all_feas)
                x = self.prelu1(self.linear1(all_feas))
                x = self.prelu2(self.linear2(x))
                x = self.linear3(x)
                score = F.softmax(x,dim=-1) + 0.00000001
                scores.append(score)

        elif not self.training:
            if self.task_type=='train':
                mem_all, _ = self.agg(mem_all, seq_all, mask_raw, mode='b')
            all_feas = torch.cat([gate*mem_all + (1.0-gate)*dev_all_mean, item_all], -1)
            all_feas = self.bn(all_feas)
            x = self.prelu1(self.linear1(all_feas))
            x = self.prelu2(self.linear2(x))
            x = self.linear3(x)
            score = F.softmax(x,dim=-1) + 0.00000001
            scores.append(score)
        elif self.task_type=='inference' and self.training:
            mem_all, _ = self.agg(mem_all, seq_all, mask_raw, mode='b')
            d_model = mem_all.size(1) // 2
            mem, mem_cat = mem_all[:, :d_model], mem_all[:, d_model:]
            # update memory
            self.emb.Mem_item.weight.data[torch.LongTensor(input['user_id'].to('cpu'))] = mem
            self.emb.Mem_cat.weight.data[torch.LongTensor(input['user_id'].to('cpu'))] = mem_cat
            return None, None

        score_avg = sum(scores) / len(scores)

        label = torch.stack([input['label'], 1-input['label']], dim=1)


        loss_sup = 0.0 # supervised loss
        loss_unsup = 0.0 # self-supervised loss
        if self.training:
            for k in range(self.K):
                loss_unsup += F.mse_loss(scores[k], score_avg)
                loss_sup += (-torch.mean(torch.log(scores[k]) * label))
        else:
            loss_sup += (-torch.mean(torch.log(scores[0]) * label))

        loss = loss_unsup * self.l + loss_sup

        return loss, score_avg

class Aggregation(nn.Module):
    def __init__(self, d_model, num_head):
        super(Aggregation, self).__init__()
        self.Q = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_uniform_(self.Q.weight, gain=1.0)
        self.K = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_uniform_(self.K.weight, gain=1.0)
        self.V = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_uniform_(self.V.weight, gain=1.0)
        self.d_h = d_model//num_head
        self.num_head = num_head
        self.d_model = d_model
        # self.ln = nn.LayerNorm(d_model)

    def forward(self, memory, items, mask, mode='agg'):
        if mode in ('only', 'aggregation','agg', 'a'):
            return self._only_aggregation_multihead(memory, items, mask)
        else:
            return self._multihead_attention(memory, items, mask)

    def _only_aggregation_multihead(self, memory, items, mask=None, residual=False):
        bs = memory.size(0)
        seq = torch.cat([memory.unsqueeze(1), items], 1) # (B, 1+T, D)
        mem_q = self.Q(memory).view(bs, -1, self.num_head, self.d_h).transpose(1,2) # (B, H, 1, d_h)
        seq_k = self.K(seq).view(bs, -1, self.num_head, self.d_h).transpose(1,2) # (B, H, 1+T, d_h)
        seq_v = seq.view(bs, -1, self.num_head, self.d_h).transpose(1, 2)  # (B, H, 1+T, d_h)

        mem, attn = self._only_aggregation_single(mem_q, seq_k, seq_v, mask) # (B, H, 1, d_h), (B, H, 1, 1+T)
        mem = mem.transpose(1,2).contiguous().view(bs, self.d_model) # (B, D)
        if residual:
            mem = torch.add(mem, memory)
        return mem, attn

    def _only_aggregation_single(self, mem_q, seq_k, seq_v, mask=None, dropout=None):
        scores = torch.matmul(mem_q, seq_k.transpose(-2,-1)) / math.sqrt(self.d_h) # (B, H, 1, 1+T)
        if mask is not None:
            scores = scores.masked_fill(mask==0, -2**32+1)
        p_attn = F.softmax(scores, dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, seq_v), p_attn # (B, H, 1, d_h), (B, H, 1, 1+T)

    def _multihead_attention(self, memory, items, mask=None, residual=True):
        bs = memory.size(0)
        seq = torch.cat([memory.unsqueeze(1), items], 1) # (B, 1+T, D)
        q = self.Q(seq).view(bs, -1, self.num_head, self.d_h).transpose(1,2) # (B, H, 1+T, d_h)
        k = self.K(seq).view(bs, -1, self.num_head, self.d_h).transpose(1,2) # (B, H, 1+T, d_h)
        v = seq.view(bs, -1, self.num_head, self.d_h).transpose(1, 2)  # (B, H, 1+T, d_h)

        seq_new, attn = self._single_attention(q, k, v, mask) # (B, H, 1+T, d_h), (B, H, 1+T, 1+T)

        seq_new = seq_new.transpose(1,2).contiguous().view(bs, -1, self.d_model) # (B, 1+T, D)

        mem = seq_new[:,0,:]

        if residual:
            mem = torch.add(mem, memory)
        return mem, attn

    def _single_attention(self, q, k, v, mask=None, dropout=None):
        scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.d_h) # (B, H, 1+T, 1+T)
        scores = scores / self.d_h ** 0.5
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -2**32+1)

        p_attn = F.softmax(scores, dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, v), p_attn # (B, H, 1+T, d_h), (B, H, 1+T, 1+T)

class Embedding(nn.Module):
    def __init__(self, num_user, num_item, num_cat, d_model=32, d_mem=128, mem_init = '2'):
        super().__init__()
        if mem_init in ('0', 'zero'):
            self.Mem_item = nn.Embedding.from_pretrained(torch.zeros(num_user, d_mem), freeze=True)
            self.Mem_cat = nn.Embedding.from_pretrained(torch.zeros(num_user, d_mem), freeze=True)
        elif mem_init in ('1', 'one'):
            self.Mem_item = nn.Embedding.from_pretrained(torch.ones(num_user, d_mem) / d_mem, freeze=True)
            self.Mem_cat = nn.Embedding.from_pretrained(torch.ones(num_user, d_mem) / d_mem, freeze=True)
        else:
            self.Mem_item = nn.Embedding(num_user, d_mem)
            nn.init.xavier_uniform_(self.Mem_item.weight, gain=1.0)
            self.Mem_cat = nn.Embedding(num_user, d_mem)
            nn.init.xavier_uniform_(self.Mem_cat.weight, gain=1.0)

        self.Item = nn.Embedding(num_item, d_model, padding_idx=0)
        nn.init.xavier_uniform_(self.Item.weight, gain=1.0)
        self.Cat = nn.Embedding(num_cat, d_model, padding_idx=0)
        nn.init.xavier_uniform_(self.Cat.weight, gain=1.0)
        self.User = nn.Embedding(num_user, d_model, padding_idx=0)
        nn.init.xavier_uniform_(self.User.weight, gain=1.0)

    def forward(self, input):
        output = {
            'mem_item': self.Mem_item(input['user_id']),
            'mem_cate': self.Mem_cat(input['user_id']),
            'item': self.Item(input['item_id']),
            'cate': self.Cat(input['cate_id']),
            'user': self.User(input['user_id']),
            'edge_seq_item': self.Item(input['edge_item_seq']),
            'edge_seq_cate': self.Cat(input['edge_cate_seq']),
            'seq_item': self.Item(input['item_seq']),
            'seq_cate': self.Cat(input['cate_seq'])

        }
        return output